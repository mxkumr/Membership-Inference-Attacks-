{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Team Details\n",
        "\n",
        "Member 1 Name: Kiruthiharan Basker <br>\n",
        "Member 1 Student Id: 7062652\n",
        "\n",
        "Member 2 Name: Manish Kumar Bala Kumar <br>\n",
        "Member 2 Student Id: 7062996\n",
        "\n",
        "Member 3 Name: Sanju Kunjumman Jacob <br>\n",
        "Member 3 Student Id: 7061424\n",
        "\n",
        "Member 4 Name: Anjali Sankar Eswaramangalath <br>\n",
        "Member 4 Student Id: 7062531"
      ],
      "metadata": {
        "id": "D1DGykQGQUlK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8v6yq1y6RHls"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "import io\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import random\n",
        "from torch.optim import lr_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuSDQutp8hk2"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4C52Rv3uhId2"
      },
      "outputs": [],
      "source": [
        "def load_shadow_dataset(DATA_PATH, N):\n",
        "\n",
        "  with open(DATA_PATH, \"rb\") as f:\n",
        "      dataset = pickle.load(f)\n",
        "\n",
        "  train_dataset = []\n",
        "  test_dataset = []\n",
        "\n",
        "  train_dataloader = []\n",
        "  test_dataloader = []\n",
        "\n",
        "  train_size = int(0.8 * len(dataset))\n",
        "  test_size = len(dataset) - train_size\n",
        "\n",
        "  for i in range(N):\n",
        "    random.shuffle(dataset)\n",
        "    train_d, test_d = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_dl = torch.utils.data.DataLoader(train_d, batch_size=64, shuffle=True, num_workers=2)\n",
        "    test_dl = torch.utils.data.DataLoader(test_d, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "    train_dataset.append(train_d)\n",
        "    test_dataset.append(test_d)\n",
        "\n",
        "    train_dataloader.append(train_dl)\n",
        "    test_dataloader.append(test_dl)\n",
        "\n",
        "  return train_dataloader, test_dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vscYvu9IfxB6"
      },
      "outputs": [],
      "source": [
        "def train_resnet(train_dataloader, test_dataloader, num_classes, num_epochs):\n",
        "    resnet_model = models.resnet34(pretrained=True)\n",
        "    resnet_model.fc = nn.Linear(resnet_model.fc.in_features, num_classes)\n",
        "    resnet_model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(resnet_model.parameters(), lr=0.001, momentum=0.9)\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "    resnet_model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        resnet_model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in train_dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = resnet_model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_dataloader)\n",
        "        train_accuracy = 100. * correct / total\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "    resnet_model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = resnet_model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    test_accuracy = 100. * correct / total\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    return resnet_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbitLrVtD9qF"
      },
      "outputs": [],
      "source": [
        "def train_mobilenet(train_dataloader, test_dataloader, num_classes, num_epochs):\n",
        "    mobilenet_model = models.mobilenet_v2(pretrained=True)\n",
        "    mobilenet_model.classifier[1] = nn.Linear(mobilenet_model.classifier[1].in_features, num_classes)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(mobilenet_model.parameters(), lr=0.001, momentum=0.9)\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "    mobilenet_model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        mobilenet_model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in train_dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = mobilenet_model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_dataloader)\n",
        "        train_accuracy = 100. * correct / total\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "    mobilenet_model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = mobilenet_model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    test_accuracy = 100. * correct / total\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    return mobilenet_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bukfpAP9n9Mc"
      },
      "outputs": [],
      "source": [
        "def generate_dataset_from_shadow_models(model, dataloader, member):\n",
        "  model.eval()\n",
        "  x = []\n",
        "  y = []\n",
        "  with torch.no_grad():\n",
        "      for images, labels in dataloader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          outputs = model(images)\n",
        "          probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "          predictions = torch.argmax(outputs, 1)\n",
        "          _, predicted = outputs.max(1)\n",
        "          for i in range(len(labels)):\n",
        "            y.append(member)\n",
        "            x.append([labels[i].item()] + [predictions[i].item()] + probabilities[i].tolist())\n",
        "\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uuq0_bLXDkl7"
      },
      "outputs": [],
      "source": [
        "# read eval data\n",
        "def load_eval_data(DATA_PATH):\n",
        "  with open(DATA_PATH, \"rb\") as f:\n",
        "      dataset = pickle.load(f)\n",
        "\n",
        "  eval_x_ds = [(item[0], item[1]) for item in dataset]\n",
        "  eval_x_dl = torch.utils.data.DataLoader(eval_x_ds, batch_size=64, shuffle=False, num_workers=2)\n",
        "  eval_y = [item[2] for item in dataset]\n",
        "  return eval_x_dl, eval_y\n",
        "\n",
        "# load target model\n",
        "def load_target_model(MODEL_PATH, num_classes, model_type=\"resnet34\"):\n",
        "\n",
        "  if model_type == \"resnet34\":\n",
        "    target_model = models.resnet34(num_classes=num_classes).to(device)\n",
        "  else:\n",
        "    target_model = models.mobilenet_v2(num_classes=num_classes).to(device)\n",
        "\n",
        "  state_dict = torch.load(MODEL_PATH, map_location=device)\n",
        "  target_model.load_state_dict(state_dict['net'])\n",
        "  return target_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A28qpYVrqZyw"
      },
      "outputs": [],
      "source": [
        "def train_random_forest(X_train, y_train):\n",
        "    t_model = RandomForestClassifier(n_estimators=50)\n",
        "    t_model.fit(X_train, y_train)\n",
        "    return t_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0rA0XXX0T8k"
      },
      "outputs": [],
      "source": [
        "# read eval data\n",
        "def load_test_data(DATA_PATH):\n",
        "  with open(DATA_PATH, \"rb\") as f:\n",
        "      dataset = pickle.load(f)\n",
        "\n",
        "  eval_x_ds = [(item[0], item[1]) for item in dataset]\n",
        "  eval_x_dl = torch.utils.data.DataLoader(eval_x_ds, batch_size=64, shuffle=False, num_workers=2)\n",
        "  return eval_x_dl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/amlm'\n",
        "task_0 = {'id':'task0_resnet34_cifar10',\n",
        "          'N_CLASS_LABELS': 10,\n",
        "          'MODEL': 'resnet34',\n",
        "          'DATASET': 'cifar10',\n",
        "          'epochs': 25,\n",
        "          'DATA_SET_PATH': f\"{BASE_PATH}/pickle/cifar10/resnet34\",\n",
        "          'OUTPUT_PATH': f\"{BASE_PATH}/shadow_0/\",\n",
        "          'TARGET_MODEL_PATH': f\"{BASE_PATH}/models/resnet34_cifar10.pth\"}\n",
        "\n",
        "task_1 = {'id':'task1_mobilenetv2_cifar10',\n",
        "          'N_CLASS_LABELS': 10,\n",
        "          'MODEL': 'mobilenetv2',\n",
        "          'DATASET': 'cifar10',\n",
        "          'epochs': 25,\n",
        "          'DATA_SET_PATH': f\"{BASE_PATH}/pickle/cifar10/mobilenetv2\",\n",
        "          'OUTPUT_PATH': f\"{BASE_PATH}/shadow_1/\",\n",
        "          'TARGET_MODEL_PATH': f\"{BASE_PATH}/models/mobilenetv2_cifar10.pth\"}\n",
        "\n",
        "task_2 = {'id':'task2_resnet34_tinyimagenet',\n",
        "          'N_CLASS_LABELS': 200,\n",
        "          'MODEL': 'resnet34',\n",
        "          'DATASET': 'tinyimagenet',\n",
        "          'epochs': 8,\n",
        "          'DATA_SET_PATH': f\"{BASE_PATH}/pickle/tinyimagenet/resnet34\",\n",
        "          'OUTPUT_PATH': f\"{BASE_PATH}/shadow_2/\",\n",
        "          'TARGET_MODEL_PATH': f\"{BASE_PATH}/models/resnet34_tinyimagenet.pth\"}\n",
        "\n",
        "task_3 = {'id':'task3_mobilenetv2_tinyimagenet',\n",
        "          'N_CLASS_LABELS': 200,\n",
        "          'MODEL': 'mobilenetv2',\n",
        "          'DATASET': 'tinyimagenet',\n",
        "          'epochs': 15,\n",
        "          'DATA_SET_PATH': f\"{BASE_PATH}/pickle/tinyimagenet/mobilenetv2\",\n",
        "          'OUTPUT_PATH': f\"{BASE_PATH}/shadow_3/\",\n",
        "          'TARGET_MODEL_PATH': f\"{BASE_PATH}/models/mobilenetv2_tinyimagenet.pth\"}\n",
        "\n",
        "configs = [task_0, task_1, task_2, task_3]\n",
        "\n",
        "def mia(task=0, stages=[0,1,2], N=1):\n",
        "  OUTPUT_PATH = configs[task]['OUTPUT_PATH']\n",
        "  N_CLASS_LABELS = configs[task]['N_CLASS_LABELS']\n",
        "  TARGET_MODEL_PATH = configs[task]['TARGET_MODEL_PATH']\n",
        "  MODEL_TYPE = configs[task]['MODEL']\n",
        "  DATA_SET_PATH = configs[task]['DATA_SET_PATH']\n",
        "\n",
        "  if 0 in stages:\n",
        "    # load shadow dataset\n",
        "    train_dataloaders, test_dataloaders = load_shadow_dataset(DATA_SET_PATH + '/shadow.p', N)\n",
        "\n",
        "    # train shadow models\n",
        "    shadow_models = []\n",
        "    for i in range(N):\n",
        "      if task == 0 or task == 2:\n",
        "        shadow_models.append(train_resnet(train_dataloaders[i], test_dataloaders[i], N_CLASS_LABELS, num_epochs=configs[task]['epochs']))\n",
        "      else:\n",
        "        shadow_models.append(train_mobilenet(train_dataloaders[i], test_dataloaders[i], N_CLASS_LABELS, num_epochs=configs[task]['epochs']))\n",
        "\n",
        "    # save shadow models\n",
        "    for i in range(N):\n",
        "      filename = f'shadow_model_{i}.sav'\n",
        "      torch.save(shadow_models[i], OUTPUT_PATH+filename)\n",
        "\n",
        "    # import shadow models\n",
        "    loaded_models = []\n",
        "    for i in range(N):\n",
        "      filename = f'shadow_model_{i}.sav'\n",
        "      loaded_models.append(torch.load(OUTPUT_PATH+filename))\n",
        "\n",
        "    # Generate attack dataset\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(N):\n",
        "      temp_x1, temp_y1 = generate_dataset_from_shadow_models(loaded_models[i], train_dataloaders[i], 1)\n",
        "      temp_x2, temp_y2 = generate_dataset_from_shadow_models(loaded_models[i], test_dataloaders[i], 0)\n",
        "\n",
        "      X = X + temp_x1 + temp_x2\n",
        "      y = y + temp_y1 + temp_y2\n",
        "\n",
        "\n",
        "    with open(OUTPUT_PATH + \"/shadow_X\", \"wb\") as fp:\n",
        "      pickle.dump(X, fp)\n",
        "\n",
        "    with open(OUTPUT_PATH + \"/shadow_Y\", \"wb\") as fp:\n",
        "      pickle.dump(y, fp)\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------------\n",
        "  if 1 in stages:\n",
        "    # Load dataset generated by shadow model\n",
        "    with open(OUTPUT_PATH + \"/shadow_X\", \"rb\") as fp:\n",
        "      X = pickle.load(fp)\n",
        "\n",
        "    with open(OUTPUT_PATH + \"/shadow_Y\", \"rb\") as fp:\n",
        "      y = pickle.load(fp)\n",
        "\n",
        "    # Create default dict\n",
        "    X_dict = {}\n",
        "    y_dict = {}\n",
        "    for i in range(N_CLASS_LABELS):\n",
        "      X_dict[i] = []\n",
        "      y_dict[i] = []\n",
        "\n",
        "    # add to dict by class\n",
        "    for i in range(len(X)):\n",
        "      X_dict[X[i][0]].append(X[i][2:])\n",
        "      y_dict[X[i][0]].append(y[i])\n",
        "\n",
        "    # train a model per class\n",
        "    attack_models = []\n",
        "    for i in range(N_CLASS_LABELS):\n",
        "      X_train =  X_dict[i]\n",
        "      y_train = y_dict[i]\n",
        "      am = train_random_forest(X_train, y_train)\n",
        "      attack_models.append(am)\n",
        "\n",
        "    # save attack models\n",
        "\n",
        "    for i in range(N_CLASS_LABELS):\n",
        "      filename = f'attack_model_{i}.sav'\n",
        "      torch.save(attack_models[i], OUTPUT_PATH+filename)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "  if 2 in stages:\n",
        "    # import attack models\n",
        "\n",
        "    attack_models = []\n",
        "    for i in range(N_CLASS_LABELS):\n",
        "      filename = f'attack_model_{i}.sav'\n",
        "      attack_models.append(torch.load(OUTPUT_PATH+filename))\n",
        "\n",
        "\n",
        "    # load target model\n",
        "    target_model = load_target_model(TARGET_MODEL_PATH, N_CLASS_LABELS, MODEL_TYPE)\n",
        "\n",
        "    # Load eval dataset\n",
        "    X_eval, y_eval = load_eval_data(DATA_SET_PATH + '/eval.p')\n",
        "    X_eval, _ = generate_dataset_from_shadow_models(target_model, X_eval, 1)\n",
        "\n",
        "    # Get predictions\n",
        "    preds = []\n",
        "    for i in range(len(X_eval)):\n",
        "      y_pred = attack_models[X_eval[i][0]].predict([X_eval[i][2:]])\n",
        "      preds.append(y_pred[0])\n",
        "\n",
        "    print(len(preds), len(y_eval))\n",
        "\n",
        "    acc = 0\n",
        "    for i in range(len(preds)):\n",
        "      if preds[i] == y_eval[i] : acc +=1\n",
        "    accuracy = acc / len(y_eval)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "    # Load and get predictions for test file\n",
        "    X_test = load_test_data(DATA_SET_PATH+'/test.p')\n",
        "    X_test, _ = generate_dataset_from_shadow_models(target_model, X_test, 1)\n",
        "\n",
        "    preds_test = []\n",
        "    for i in range(len(X_test)):\n",
        "      y_pred = attack_models[X_test[i][0]].predict([X_test[i][2:]])\n",
        "      preds_test.append(y_pred[0])\n",
        "\n",
        "    np_preds = np.asarray(preds_test, dtype=np.int32)\n",
        "    np.save(configs[task]['id'] + '.npy', np_preds)\n",
        "\n"
      ],
      "metadata": {
        "id": "plURvJH8C3yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mia(task = 0, stages = [0, 1, 2], N = 1)\n",
        "mia(task = 1, stages = [0, 1, 2], N = 1)\n",
        "mia(task = 2, stages = [0, 1, 2], N = 1)\n",
        "mia(task = 3, stages = [0, 1, 2], N = 1)"
      ],
      "metadata": {
        "id": "ZlJD9_6BJUTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pteWQ1k1krv5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}